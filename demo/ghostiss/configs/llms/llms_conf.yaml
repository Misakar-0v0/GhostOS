# 详细配置逻辑见 ghostiss.framework.llms.llms::LLMsYamlConfig
# 这个文件是配合单元测试的, 请不要修改.
services:
  - name: moonshot
    # driver: use default
    base_url: https://api.moonshot.cn/v1
    token: $MOONSHOT_API_KEY
  - name: openai
    base_url: https://api.openai.com/v1
    token: $OPENAI_API_KEY
    proxy: $OPENAI_PROXY
  - name: local
    base_url: http://localhost:11434/v1
    token: ollama
#    proxy: $OPENAI_PROXY
# 默认的 api.
default:
  service: moonshot
  model: moonshot-v1-32k
models:
  moonshot-v1-8k:
    service: moonshot
    model: moonshot-v1-8k
  moonshot-v1-32k:
    service: moonshot
    model: moonshot-v1-32k
  moonshot-v1-128k:
    service: moonshot
    model: moonshot-v1-128k
  gpt-3.5-turbo:
    service: openai
    model: gpt-3.5-turbo
  gpt-4:
    service: openai
    model: gpt-4
  gpt-4-turbo:
    service: openai
    model: gpt-4-turbo
  gpt-4o:
    service: openai
    model: gpt-4o
  qwen2-72b:
    service: local
    model: qwen2:72b
    timeout: 80
    request_timeout: 40
    max_tokens: 8192
  codestral-22b:
    service: local
    model: codestral:22b
    timeout: 80
    request_timeout: 80
    max_tokens: 32768
  llama3-1-70b:
    service: local
    model: llama3.1:70b
    timeout: 80
    request_timeout: 80
    max_tokens: 131072
  llama3-1-8b:
    service: local
    model: llama3.1:8b
    timeout: 80
    request_timeout: 80
    max_tokens: 131072


#  gemma2-27b:
#    service: local
#    model: gemma2:27b
#    timeout: 80
#    request_timeout: 80
#    max_tokens: 8192

#  codegeex4-9b:
#    service: local
#    model: codegeex4:9b
#    timeout: 80
#    request_timeout: 40
#    max_tokens: 131072

