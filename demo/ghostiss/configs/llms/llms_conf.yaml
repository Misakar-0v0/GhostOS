# 详细配置逻辑见 ghostiss.framework.llms.llms::LLMsYamlConfig
# 这个文件是配合单元测试的, 请不要修改.
services:
  - name: moonshot
    # driver: use default
    base_url: https://api.moonshot.cn/v1
    token: $MOONSHOT_API_KEY
  - name: openai
    base_url: https://api.openai.com/v1
    token: $OPENAI_API_KEY
    proxy: $OPENAI_PROXY
  - name: anthropic
    token: $ANTHROPIC_API_KEY
    proxy: $OPENAI_PROXY
    base_url: https://api.anthropic.com/v1
  - name: deepseek
    token: $DEEPSEEK_API_KEY
    base_url: https://api.deepseek.com/beta
  - name: local
    base_url: http://localhost:11434/v1
    token: ollama
#    proxy: $OPENAI_PROXY
# 默认的 api.
default:
  service: deepseek
  model: deepseek/deepseek-coder
#  service: openai
#  model: gpt-4o
models:
  moonshot-v1-8k:
    service: moonshot
    model: moonshot-v1-8k
  moonshot-v1-32k:
    service: moonshot
    model: moonshot-v1-32k
  moonshot-v1-128k:
    service: moonshot
    model: moonshot-v1-128k
  gpt-3.5-turbo:
    service: openai
    model: gpt-3.5-turbo
  gpt-4:
    service: openai
    model: gpt-4
  gpt-4-turbo:
    service: openai
    model: gpt-4-turbo
  gpt-4o:
    service: openai
    model: gpt-4o
  claude-3-5-sonnet: # 200K context window, 3$/M input, 3.75$/M cache write, 0.3$/M cache read, 15$/M output
    service: anthropic
    model: claude-3-5-sonnet-20240620
  claude-3-haiku: # 200K context window, 0.25$/M input, 0.3$/M cache write, 0.03$/M cache read, 1.25$/M output
    service: anthropic
    model: claude-3-haiku-20240307
  deepseek-chat: # 128k context window, 4k output window. 1Y/M input, 0.1Y/M cache hit, 2Y/M output
    service: deepseek
    model: deepseek/deepseek-chat
  deepseek-coder: # 128k context window, 8k output window. 1Y/M input, 0.1Y/M cache hit, 2Y/M output
    service: deepseek
    model: deepseek/deepseek-coder
  qwen2-72b:
    service: local
    model: qwen2:72b
    timeout: 80
    request_timeout: 40
    max_tokens: 8192
  codestral-22b:
    service: local
    model: codestral:22b
    timeout: 80
    request_timeout: 80
    max_tokens: 32768
  llama3-1-70b:
    service: local
    model: llama3.1:70b
    timeout: 80
    request_timeout: 80
    max_tokens: 131072
  llama3-1-8b:
    service: local
    model: llama3.1:8b
    timeout: 80
    request_timeout: 80
    max_tokens: 131072


#  gemma2-27b:
#    service: local
#    model: gemma2:27b
#    timeout: 80
#    request_timeout: 80
#    max_tokens: 8192

#  codegeex4-9b:
#    service: local
#    model: codegeex4:9b
#    timeout: 80
#    request_timeout: 40
#    max_tokens: 131072

